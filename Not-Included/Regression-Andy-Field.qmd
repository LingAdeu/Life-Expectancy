---
title: "Regression"
format: html
---

# 1 An introduction to regression

-   Regression is a way of predicting an outcome variable from one predictor variable (**simple regression**) or several variables (**multiple regression**)

-   General equation: $$
    \text{outcome}_i = \text{(model)} + \text{error}_i
    $$

-   Mathematical equation: $$
    Y_i=(b_0+b_1X_i)+\epsilon_i
    $$

-   A model consists of (1) slope (gradient) of the line (denoted by $b_1$), and (2) intercept (point at which the line crosses the vertical axis of the graph, denoted by $b_0$).

    -   Both $b_0$ and $b_1$ are known as **regression coefficients**

    -   The $\epsilon_i$ or error is the residual (real data point - predicted data point). In regression, the difference is called residual, not deviation, although both of which mean the same thing.

# 2 Method of least square

-   The method of least square: a way of finding the line that best fits the data.

```{r, message=F, warning=F}
library(tidyverse)
library(patchwork)
library(broom)
```

```{r}
df <- read_csv("D:/PROGRAMMING - LABS/Statistics-for-Business-Regression/datasets/ugtests.csv", show_col_types = F)

# subset
top10 <-
  head(df[, c("Yr3", "Final")], 10)
```


```{r}
# fit the model
model <-
  lm(Final ~ Yr3, data=top10)

# get residual and predicted values
residuals <- residuals(model)
predicted <- predict(model)

# make the plot
ggplot(top10, aes(x = Yr3, y = Final)) +
  geom_point(size = 2, shape=21, color="steelblue") + 
  geom_smooth(method = "lm", formula = "y ~ x", se = F,
              color="steelblue") +
  geom_segment(aes(xend = Yr3, yend = predicted),
               linetype = "dashed") +
  theme_bw() + 
  labs(title="Residuals for 10 obs", x= "Year 3")
  
```

-   The line of best fit is known as a **regression line** or **regression model**.

# 3 Assessing the goodness of fit: Sums of squares, $R$, and $R^2$

-   To asses the line of best fit, use this formula: $$
    \text{deviation}=\Sigma(\text{observed - model})^2
    $$

-   R-squared ($R^2$) $$
    R^2=\frac{SS_M}{SS_T}
    $$

**Note**:

-   $SS_M$: model sum of squares $\rightarrow$ degree of inaccuracy when t6he best model is fitted to the data.

-   $SS_T$: total sum of squares

-   The square root of $R$-squared = Pearson's correlation coefficient ($R$).

```{r}
p1 <- 
  ggplot(top10, aes(x = Yr3, y = Final)) +
  geom_point(size = 2, color = "steelblue",
             shape=19) + 
  geom_smooth(method = "lm", formula = "y ~ x", se = F,
              color="steelblue") +
  geom_hline(yintercept=mean(predicted)) +
  geom_segment(aes(xend = Yr3, yend = predicted),
               linetype = "dashed") +
  theme_bw() + 
  labs(title = "Residual sum of square (SSR)",
       tag="A", x= "Score: Year 3") +
  theme(plot.title=element_text(size=12))

p2 <- 
  ggplot(top10, aes(x = Yr3, y = Final)) +
  geom_point(size = 2, color = "steelblue",
             shape=19) + 
  geom_hline(yintercept=mean(predicted)) +
  geom_segment(aes(xend = Yr3, yend = mean(predicted)),
               linetype = "dashed") +
  theme_bw() + 
  labs(title = "Total sum of square (SST)", 
       tag="B", x= "Score: Year 3") +
  theme(plot.title=element_text(size=12))

p3 <- ggplot(top10, aes(x = Yr3, y = Final)) +
  geom_point(size = 2, color = "steelblue",
             shape=19) + 
  geom_hline(yintercept=mean(predicted)) +
  geom_smooth(method="lm", formula="y~x", se=F) +
  geom_segment(aes(y=predicted, xend = Yr3, 
                   yend = mean(predicted)), 
               linetype = "dashed") +
  theme_bw() + 
  labs(title="Model sum of square (SSM)",
       tag="C", x= "Score: Year 3") +
  theme(plot.title=element_text(size=12))

(p1 | p2) / p3
```

-   **Note**:

    -   **A**: $SS_R$ uses the differences between the observed data and the regression line.

    -   **B**: $SS_T$ uses the differences between the observed data and the mean value of $\text{Final}$.

    -   **C**: $SS_M$ uses the differences between the mean value of $\text{Final}$ and the regression line.

-   Differences between $R$ and $R^2$:

    -   $R$ (Pearson's correlation coefficient) provides a good estimate of the overall fit of the regression model

    -   $R^2$ provide a good gauge of the substantive size of the relationship.

-   In addition to $R$ and $R^2$, another way of assessing a model is by using the $F$-test.

    -   $F$-test: a comparison between model and error in the model

    $$
    F=\frac{MS_M}{MS_R}
    $$

    -   Note:

        -   $MS_M$: mean square of the model

        -   $MS_R$: mean square of the residuals

        -   $MS$ is the average sums of squares ($SS_{\text{mean}}$)

# 4 General procedure for regression in R

```{r}
# import dataset
album1 <-
  read.delim("D:/PROGRAMMING - LABS/Statistics-for-Business-Regression/datasets/Album Sales 1.dat", header=T)

# display top rows
album1[1:5,]
```

```{r}
# fit the model
newModel <- lm(sales ~ adverts, data=album1)

# display result in tidy format
broom::tidy(newModel)
```

```{r}
# get the r-squared and its assoc stats
broom::glance(newModel)
```

# 5 Interpreting a simple regression

## 5.1 Overall fit of the object model

```{r}
# display result (default)
summary(newModel)
```

-   Because there is only one predictor (`adverts`), this value represents the square of the simple correlation between advertising and album sales. To get the $R$, we only need to find the $\sqrt{R^2}$: 0.578

```{r}
sqrt(0.3346)
```

-   The $R^2$ = 0.58 tells that advertising expenditure can account for 33.5% of the variation in album sales.

-   This also means that 67% of the variation in album sales can not be explained by advertising alone so other variables have have influence as well.

-   The results of an analysis of variance (ANOVA):

```         
F-statistic: 99.59 on 1 and 198 DF,  p-value: < 2.2e-16
```

-   It tells about the *F*-ratio and the *p*-value: *F* is 99.59 which is significant at *p* \< .001$^4$ $\rightarrow$ there is less than 0.1% chance that *F*-ratio this large would happen if the null hypothesis were true.
-   Overall, it means the overall model perform significantly well.

## 5.2 Using the model

Make a prediction about album sales by using the model

$$
\begin{align}
\text{album sales}_i &= b_0 + b_1 \times \text{advertising budget}_i \\
\text{album sales}_i &= 134.14 + (0.096 \times \text{advertising budget}_i) \\
&= 134.14 + (0.96 \times 100) \\
&= 143.74
\end{align}
$$

**Note**:

-   $b_0$: intercept of the model

-   $b_1$: the slope

# 6 Multiple regression: The basics

Remember that 67% of variation remains unexplained so the model is extended to incorporate another variable.

$$
\text{album sales}_i = (b_0+b_1 \times \text{advertising budget}_i + b_2 \times \text{airplay} + \epsilon_i)
$$

## 6.1 How to do multiple regression using R

```{r}
# import dataset
album2 <- 
  read.delim("D:/PROGRAMMING - LABS/Statistics-for-Business-Regression/datasets/Album Sales 2.dat", header=T)

# display dataset
album2
```

```{r}
# plotting
library(scatterplot3d)

scatterplot3d(album2[,c("adverts", "sales", "airplay")],
              pch=21, grid=T, box=T, type="h")
```


```{r}
# fit the model: simple LM
albumSales.2 <- lm(sales ~ adverts, data=album2)

# display model: simple LM
tidy(albumSales.2)
```

$$
\text{album sales}_i = 134.13 + (0.09 \times \text{advertising budget}_i)
$$

```{r}
# fit the model: multiple linear regression
albumSales.3 <- 
  lm(sales ~ adverts + airplay + attract, data=album2)

# display the model
tidy(albumSales.3)
```

$$
\begin{align}
\text{album sales}_i &= b_0+b_1 \times \text{advertising budget}_i + b_2 \times \text{airplay} + \epsilon_i \\
\text{album sales}_i &=-19.53 + 0.11 \times \text{advertising budget}_i + 4.7 + \epsilon_i
\end{align}
$$

## 6.2 Interperting the basic multiple regression

```{r}
# one predictor: adverts
tidy(albumSales.2)
```

```{r}
# get related stats
glance(albumSales.2)
```

```{r}
# multiple predictors: adverts, airplay, attract
tidy(albumSales.3)
```

```{r}
# get related stats
glance(albumSales.3)
```

The first model results in 33.5% variation in album sales but when two other variables included in the equation, the value increases to 65% of variance in album sales. Then, if advertising accounts for 33.5%, attractiveness and radio play account for an additional 33.0%.

> The fit of regression model can be assessed with the model fit. Look for the $R^2$ to tell you the proportion of variance explained by the model. Multiply this value by 100 to give you the percentage of variance explained by the model.

## 6.3 Model parameters

$$
\begin{align}
\text{album sales}_i &= b_0 +b_1* \text{advertising budget} + b_2* \text{airplay} + b_3* \text{attractiveness} \\
&= -26.61 + (0.08 * \text{advertising budget}) + (3.37 * \text{airplay}) + (11.09 * \text{attractiveness1})
\end{align}
$$

-   $b_0$: Positive or negative $b$-values represents the positive or negative relationship between predictors and the outcome.

-   The equation suggests that there is a negative relationship between the predictors and the outcome: When the advertising budget decreases, album sales increases, plays on the radio and attractiveness bond increase.

-   They tell us what degree each predictor affects the outcome *if the effects of all other predictors are held constant*:

    -   **Advertising budget** ($b$ = 0.085): As advertising budget increases by one unit, album sales increase by 0.085 units. Both variables were measured in thousands; therefore, for every 1000 pound sterling more spent on advertising, an extra 0.085 thousand albums (85 albums) were sold. The interpretation is true only if the effects of attractiveness band and airplay are held constant.

    -   **Airplay** ($b$ = 3.367): The value indicates that as the number of plays on radio in the week before release increase by one, album sales increase by 3.367 units. Therefore, every additional pay of song in radio (in the week before release) is associated with an extra 3.367 thousand albums (3367 albums) being sold. This interpretation is true only if the effects of attractiveness of the band and advertising are held constant.

    -   **Attractiveness** ($b$ = 11.086): This value indicates that a band rated one unit higher on the attractiveness scale can expect additional album sales of 11.086 units. Therefore, every unit increase in the attractiveness of the band is associated with an extra 11.086 thousand albums (11,086 albums) being sold. This interpretation is true only if the effects of radio airplay and advertising are held constant.

```{r}
# get confidence intervals
confint(albumSales.3)
```

-   A good model will have a small confidence interval

-   A very bad model will have confidence intervals that cross zero.

## 6.4 Testing the accuracy of regression model

### 6.4.1 Outliers and influential cases

-   **Outliers**: Residuals can be obtained with the `resid()` function, standardized with `rstandard()` function and studentized residulas with `rstudent()` function.

-   **Influential cases**: Cook's distances can be obtained with the `cooks.distance()` function, DFBeta with `dfbeta()`

```{r}
album2$residuals <- resid(albumSales.3)
album2$standardized.residuals <- rstudent(albumSales.3)
album2$cook.distance <- cooks.distance(albumSales.3)
album2$dfbeta <- dfbeta(albumSales.3)
album2$dffit <- dffits(albumSales.3)
album2$leverage <- hatvalues(albumSales.3)
album2$covariance.ratios <- covratio(albumSales.3)

album2
```
### 6.4.2 Assessing assumption of independence

```{r, message=F}
library(car)
durbinWatsonTest(albumSales.3)
```
If the value is less than 1 or greater than 3, there should be a warning.

### 6.4.3 Assessing assumption of no multicollinearity

```{r}
# variance inflation factor
vif(albumSales.3)
```
```{r}
mean(vif(albumSales.3))
```

-   If the largest VIF is > 10 then there is cause for concern.

-   If the avg VIF is substantially greater than 1 then the regression may be biased.

-   Tolerance below 0.1 indicates a serious problem.

-   Tolerance below 0.2 indicates a potential problem.


- Current VIF values are all well below 10 and the tolerance statistics all well above 0.2. Also, the average VIF is very close to 1. Based on these measures, we can safely conclude there is no collinearity within our data.






```{r}
# Load the ggplot2 package
library(ggplot2)

# Generate some example data
x <- rnorm(100)
y <- 2 * x + rnorm(100)

# Fit a linear regression model
model <- lm(y ~ x)

# Calculate the residuals
residuals <- resid(model)

# Generate a theoretical normal distribution
norm_dist <- rnorm(n = length(residuals), mean = mean(residuals), sd = sd(residuals))

# Create a data frame with the residuals and the normal distribution
df <- data.frame(residuals = residuals, norm_dist = norm_dist)

# Create a Q-Q plot using ggplot2
ggplot(df, aes(sample = residuals)) + 
  geom_point(aes(x=x, y = norm_dist)) + 
  geom_abline(intercept = mean(residuals), slope = sd(residuals)) +
  labs(x = "Residuals", y = "Theoretical normal distribution")

```



```{r}
# Load the "ggplot2" library
library(ggplot2)

# Load the "car" library (for the "residualPlots()" function)
library(car)

# Load the "mtcars" dataset
data(mtcars)

# Fit a multiple regression model with mpg as the outcome variable and cyl, disp, hp, and wt as predictors
model_gpt <- lm(mpg ~ cyl + disp + hp + wt, data = mtcars)

# Create a residual plot using the "residualPlots()" function from the "car" library
residualPlots(model_gpt)

```



```{r}
residualPlots(model)
```













